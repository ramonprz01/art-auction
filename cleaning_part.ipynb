{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning the [IFAR](http://www.ifar.org/catalogues_raisonnes.php?alpha=&searchtype=artist&published=1&inPrep=1&artist=&author=)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modules used throughout the document\n",
    "\n",
    "The following packages and raw python is all you will need to clean data from IFAR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load your data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once your spider finished collecting the data from IFAR, load it into memory using the JSON package and the syntax. You will be loading the data as a list full of python dictionaries. The first one at index 0 is the very first page we scraped, while the rest is a combination of the links to the published, and in-progress work of each artist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "json_file = open('your path/data.json', 'r')\n",
    "data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the types of first page and the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(data), type(data[0]), type(data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the main page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's extract all of the variables in the main webpage first by selecting the first item in our list and then key for every column we are interested in (all of them).\n",
    "\n",
    "```python\n",
    "data[our first site]['the key we need']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the variables of the first page\n",
    "name = data[0]['name'].copy()\n",
    "links = data[0]['link'].copy()\n",
    "birth_year = data[0]['birth_year'].copy()\n",
    "birth_place = data[0]['birth_place'].copy()\n",
    "death_year = data[0]['death_year'].copy()\n",
    "death_place = data[0]['death_place'].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Links are not immediately available with the full length required to be typed directly into a search bar so we'll get the full link by looping through each item in our links list, combining the first part of the link with the rest, and then appending the new values to a new list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the full links of the second pages before creating the dataframe\n",
    "full_links = []\n",
    "for link in links:\n",
    "    full = 'http://www.ifar.org/' + link\n",
    "    full_links.append(full)\n",
    "\n",
    "# Check that the variables was created successfully by verifying the lenght and content\n",
    "len(full_links), full_links[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create our first dataframe to see whether we need to do some further cleaning in the first page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the first dataset containing all of page 1\n",
    "# Zip a list of lists for the data and pass in the column names we would like to use\n",
    "page_one = pd.DataFrame(list(zip(name, full_links, birth_year, birth_place, death_year, death_place)),\n",
    "                          columns = ['names', 'links', 'birth_year', 'birth_place', 'death_year', 'death_place'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the first couple of rows of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_one.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to get rid of the html code wrapped around the values we are interested in. Let's create a function for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html_tags(text):\n",
    "    \"\"\"\n",
    "    Remove the html tags and keep the values you want.\n",
    "    \"\"\"\n",
    "    import re\n",
    "    clean = re.compile('<.*?>')\n",
    "    return re.sub(clean, '', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use the function we just created with every value in our columns, append the valus to a new list, and the substitute the list as a pandas Series in the respective columns of our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean all of the variables in the first page and add them again to the dataset\n",
    "\n",
    "# Create the lists you will be appending the clean values to\n",
    "clean_byear = []\n",
    "clean_bplace = []\n",
    "clean_dyear = []\n",
    "clean_dplace = []\n",
    "\n",
    "# Loop over each variable, clean the values, and append them to your new lists\n",
    "\n",
    "for by in page_one['birth_year']:\n",
    "    f = remove_html_tags(by)\n",
    "    clean_byear.append(f)\n",
    "    \n",
    "for bp in page_one['birth_place']:\n",
    "    q = remove_html_tags(bp)\n",
    "    clean_bplace.append(q)\n",
    "    \n",
    "for dy in page_one['death_year']:\n",
    "    e = remove_html_tags(dy)\n",
    "    clean_dyear.append(e)\n",
    "    \n",
    "for dp in page_one['death_place']:\n",
    "    w = remove_html_tags(dp)\n",
    "    clean_dplace.append(w)\n",
    "\n",
    "    \n",
    "# Add your new lists to their respective variables in the dataframe\n",
    "\n",
    "page_one['birth_year'] = pd.Series(clean_byear)\n",
    "page_one['birth_place'] = pd.Series(clean_bplace)\n",
    "page_one['death_year'] = pd.Series(clean_dyear)\n",
    "page_one['death_place'] = pd.Series(clean_dplace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that the variables were cleaned correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_one.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second set of webpages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a copy of the rest of the dataset and exclude the first page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_part = data[1:].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have two kinds of dictionaries:\n",
    "- One containing two key/value pairs that correspond to the data in the second page\n",
    "- Another one with key/value pairs corresponding to the third page with the information we need\n",
    "\n",
    "Let's inspect both of these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_part[1], second_part[100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now create our dataframe with the information in the second set of sites we crawled.\n",
    "\n",
    "We will need to look over the dictionaries with only two key/value pairs. To do this we will first check whether the dictionary contains the unique key called _artist_one_ and if it does, we will append the the artist name and the _box_info_ values into two separate lists before creating the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your lists\n",
    "\n",
    "artist_one = []\n",
    "box_info = []\n",
    "\n",
    "# Go over all of the items in the second part and extract the information in the second pages\n",
    "\n",
    "for i in second_part:\n",
    "    if 'artist_one' in i.keys(): # Since we are interested in a matching value we apply the method .keys to each dictionary\n",
    "        artist_one.append(i['artist_one']) # append the corresponding value to each variable\n",
    "        box_info.append(i['box_info'])\n",
    "    else:\n",
    "        artist_one.append(np.nan) # nan if empty\n",
    "        box_info.append('nan')\n",
    "\n",
    "# Check that the lenght matches\n",
    "\n",
    "len(artist_one), len(box_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create the dataframe for our second set of websites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_two = pd.DataFrame(list(zip(artist_one, box_info)), columns=['artist_one', 'box_info'])\n",
    "page_two.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's clean the box_info variable by looping over each item, stripping any form of whitespace, and then adding the new element to a list and the new list back to the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the box\n",
    "\n",
    "box_new = []\n",
    "\n",
    "for i in page_two.box_info:\n",
    "    tempora = []\n",
    "    if i != 'nan':\n",
    "        for s in i:\n",
    "            gen = s.strip() # remove the whitespace\n",
    "            tempora.append(gen) # append to new list\n",
    "        join_them = list(filter(None, tempora)) # clear the blank elements from the box info\n",
    "        box_new.append(join_them) # append clean list to list\n",
    "    else:\n",
    "        box_new.append(np.nan) # if element is null append nan to the list\n",
    "\n",
    "\n",
    "len(box_new), box_new[59:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the completed the dataset for the second set of websites. There is further cleaning procedures we could take but since the variables of interest are in the following pages, we will leave the set of pages as is for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_two['box_info'] = pd.Series(box_new)\n",
    "page_two.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third set of web pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the same fashion we created we started the process for the second set of websites, let's clean the third one as well.\n",
    "\n",
    "Create your blank lists, loop over the dictionaries and extract the values we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your lists\n",
    "\n",
    "artist_two = []\n",
    "column_names = []\n",
    "content = []\n",
    "\n",
    "# Loop over each dictionary\n",
    "\n",
    "for i in second_part:\n",
    "    if 'artist_two' in i.keys(): # find the key in the dictionary\n",
    "        artist_two.append(i['artist_two']) # match the corresponding values to each list \n",
    "        column_names.append(i['column_names'])\n",
    "        content.append(i['content'])\n",
    "    else:\n",
    "        artist_two.append(np.nan) # if null append np.nan to your lists\n",
    "        column_names.append(np.nan)\n",
    "        content.append(np.nan)\n",
    "\n",
    "# Check that the lenght matches\n",
    "len(artist_two), len(column_names), len(content)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Create a new dataframe with your third set of websites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_three = pd.DataFrame(list(zip(artist_two, column_names, content)),\n",
    "                         columns = ['artist', 'column_names', 'content'])\n",
    "page_three.tail() # check the lenght and the content of the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our final dataset, let's get rid of the NaN values.\n",
    "\n",
    "- First check if there are any NaN in the dataset\n",
    "- Drop the rows where all values are NaN\n",
    "- Reset your index\n",
    "\n",
    "Notice that the reason why we have so many values in this dataset when compared to the amount of values in the main webpage is because we now have an artist for every work that was published or is still in-progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First check if there are any\n",
    "page_three.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all of the NaN values\n",
    "page_three = page_three.dropna(how='all')\n",
    "page_three.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset your index\n",
    "page_three = page_three.reset_index(drop=True)\n",
    "page_three.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you noticed above when we checked for NaN values, our artist variable had more NaNs than the rest. Let's inspect those values to see if we missed something or if the data was simply unavailable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_three[page_three.artist.isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean the rest of the NaN values in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_three = page_three[pd.notnull(page_three['artist'])]\n",
    "page_three.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reset the index again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_three = page_three.reset_index(drop=True)\n",
    "page_three.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(page_three)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(page_three.isna().any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have all of the values we need but we still need to take care of a few things. We strill need to:\n",
    "\n",
    "- Take the 'Artist: ' out of the artist column\n",
    "- Split the column names and make them our variables\n",
    "- Clean the content variable\n",
    "\n",
    "\n",
    "Let's continue by taking the 'Artist: ' out of our artist variable.\n",
    "\n",
    "1. Create an empty list\n",
    "2. Find the word in the string\n",
    "3. Take it out and create a sublist\n",
    "4. Clean the sublist\n",
    "5. Append the artist name to the new list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artist_three = []\n",
    "for i in page_three.artist:\n",
    "    if 'Artist: ' in str(i): # Check if the string has 'Artist: ' in it\n",
    "        gen = i.split('Artist: ') # Separate it from the string\n",
    "        artist_three.append(gen[1].strip()) # Append the cleaned artist name to our new list \n",
    "    else:\n",
    "        artist_three.append(np.nan)\n",
    "    \n",
    "len(artist_three), artist_three[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a similar process with the other two columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the new columns\n",
    "cols_news = []\n",
    "for i in page_three.column_names: # For every list in our column\n",
    "    tempora = []\n",
    "    if i != np.nan: # Althought there are no nans in the dataset it can pay off in the future to be extra cautious\n",
    "        for s in i: # For every element in our list\n",
    "            gen = s.strip() # Clean spaces and assign a variable to it\n",
    "            tempora.append(gen) # Attach it to a temporary list\n",
    "        cols_news.append(tempora) # Attach it to our new col\n",
    "    else:\n",
    "        cols_news.append(np.nan)\n",
    "    \n",
    "len(cols_news), cols_news[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your new list\n",
    "new_content = []\n",
    "\n",
    "for i in page_three.content: # loop over the variable of interest\n",
    "    tempora = [] # create a temporary list\n",
    "    if i != np.nan: # If the element is not a NaN\n",
    "        for s in i: # Select each element\n",
    "            gen = remove_html_tags(s) # Remove the tags in it\n",
    "            sus = gen.strip() # Clear any whitespace\n",
    "            tempora.append(sus) # Attach it to our new list\n",
    "        join_them = list(filter(None, tempora)) # Filter out empty lists\n",
    "        new_content.append(tempora) # Append to our final list\n",
    "    else:\n",
    "        new_content.append(np.nan)\n",
    "    \n",
    "len(new_content), new_content[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "double_list = list(zip(cols_news, new_content))\n",
    "double_list[100][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Publications and work in process both have different variables in them. To make sure we are looking at the correct one, we will loop over each variable, check whether an element exists, and classify it as:\n",
    "\n",
    "- 1 for Publications\n",
    "- 2 for work in progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pub_or_prep = []\n",
    "for i in cols_news:\n",
    "    if 'Item Title:' in i:\n",
    "        pub_or_prep.append(1)\n",
    "    else:\n",
    "        pub_or_prep.append(2)        \n",
    "    \n",
    "pub_or_prep[-25:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now combine our new variables into a dataframe. Remember that our column names and the content are now zipped together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dos_clean = pd.DataFrame(list(zip(artist_three, pub_or_prep, double_list)),\n",
    "                         columns = ['artist', 'pub_or_prep', 'col_names_content'])\n",
    "\n",
    "dos_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final cleaning\n",
    "In order to take the columns out and match an element of the columns with an element of the content, we'll need to create a function for this to match the indext of one with the value of the other.\n",
    "\n",
    "We'll need to pass through it three things:\n",
    "\n",
    "- A Series or vector\n",
    "- The name of the column\n",
    "- The empty list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_columns(series, name, new_list): # arguments of our function\n",
    "    for i in series: # go through the series\n",
    "        if name in i[0]: # if the name of a variable is in the first list of an element in our series\n",
    "            new_list.append(i[1][i[0].index(name)]) # take the index of that element and take a matching value from the second list of that element\n",
    "        else:\n",
    "            new_list.append(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty lists for our variables\n",
    "artist = []\n",
    "item_t = []\n",
    "author = []\n",
    "author_s = []\n",
    "isbn = []\n",
    "imprint = []\n",
    "language = []\n",
    "size = []\n",
    "pages = []\n",
    "illustrations = []\n",
    "concordance = []\n",
    "bibliography = []\n",
    "index = []\n",
    "exhibition_list = []\n",
    "cronology = []\n",
    "ind_entr_cont = []\n",
    "public_note = []\n",
    "\n",
    "\n",
    "# Apply the functions with the required arguments\n",
    "\n",
    "gen_columns(series = dos_clean.col_names_content, name = 'Artist:', new_list = artist)\n",
    "gen_columns(series = dos_clean.col_names_content, name = 'Item Title:', new_list = item_t)\n",
    "gen_columns(series = dos_clean.col_names_content, name = 'Author:', new_list = author)\n",
    "gen_columns(series = dos_clean.col_names_content, name = 'Author(s):', new_list = author_s)\n",
    "gen_columns(series = dos_clean.col_names_content, name = 'ISBN:', new_list = isbn)\n",
    "gen_columns(series = dos_clean.col_names_content, name = 'Imprint:', new_list = imprint)\n",
    "gen_columns(series = dos_clean.col_names_content, name = 'Language:', new_list = language)\n",
    "gen_columns(series = dos_clean.col_names_content, name = 'Size:', new_list = size)\n",
    "gen_columns(series = dos_clean.col_names_content, name = 'Pages:', new_list = pages)\n",
    "gen_columns(series = dos_clean.col_names_content, name = 'Illustrations:', new_list = illustrations)\n",
    "gen_columns(series = dos_clean.col_names_content, name = 'Concordance:', new_list = concordance)\n",
    "gen_columns(series = dos_clean.col_names_content, name = 'Bibliography:', new_list = bibliography)\n",
    "gen_columns(series = dos_clean.col_names_content, name = 'Index:', new_list = index)\n",
    "gen_columns(series = dos_clean.col_names_content, name = 'Exhibition List:', new_list = exhibition_list)\n",
    "gen_columns(series = dos_clean.col_names_content, name = 'Chronology:', new_list = cronology)\n",
    "gen_columns(series = dos_clean.col_names_content, name = 'Individual Entries Contain:', new_list = ind_entr_cont)\n",
    "gen_columns(series = dos_clean.col_names_content, name = 'Public Note:', new_list = public_note)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that the variables were created correctly by checking the content of our new lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artist[:3], item_t[:3], author[:3], author_s[:3], isbn[:3], imprint[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language[:3], size[:3], pages[:3], illustrations[:3], concordance[:3], bibliography[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index[:3], exhibition_list[:3], cronology[:3], ind_entr_cont[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a copy of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = dos_clean.copy()\n",
    "df_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop the combined variable, col_names_content, and then add the new variables to the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_final.drop(columns='col_names_content')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final['artist_2'] = pd.Series(artist)\n",
    "df_final['item_t'] = pd.Series(item_t)\n",
    "df_final['author'] = pd.Series(author)\n",
    "df_final['author_s'] = pd.Series(author_s)\n",
    "df_final['isbn'] = pd.Series(isbn)\n",
    "df_final['imprint'] = pd.Series(imprint)\n",
    "df_final['language'] = pd.Series(language)\n",
    "df_final['size'] = pd.Series(size)\n",
    "df_final['pages'] = pd.Series(pages)\n",
    "df_final['illustrations'] = pd.Series(illustrations)\n",
    "df_final['concordance'] = pd.Series(concordance)\n",
    "df_final['bibliography'] = pd.Series(bibliography)\n",
    "df_final['index'] = pd.Series(index)\n",
    "df_final['exhibition_list'] = pd.Series(exhibition_list)\n",
    "df_final['cronology'] = pd.Series(cronology)\n",
    "df_final['ind_entr_cont'] = pd.Series(ind_entr_cont)\n",
    "df_final['public_note'] = pd.Series(public_note)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the variable artist_2 repeats elements in the artist column, let's get rid of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_final.drop(columns='artist_2')\n",
    "df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_final.author_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last bit of cleaning we will do is with the columns containig multiple authors and languages. We will first split the elements in the columns by their delimiter ';'. Then we will strip any whitespace in between them and proceed to append them to a new list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new list\n",
    "\n",
    "new_auths = []\n",
    "\n",
    "\n",
    "for i in df_final.author_s: # loop over every element of the variable\n",
    "    tmp = []\n",
    "    if pd.notna(i) == True: # if the element is not NaN\n",
    "        val = i.split(';') # Split it by ';'\n",
    "        for s in val: # For those variables we just splitted\n",
    "            val_2 = s.strip().strip('&amp').strip('nbsp\\r\\n').strip() # clean them\n",
    "            tmp.append(val_2) # append them to a temporary list\n",
    "        join_them = list(filter(None, tmp)) # filter out the empty elements\n",
    "        new_auths.append(join_them) # append back to main list\n",
    "    else:\n",
    "        new_auths.append(np.nan)\n",
    "        \n",
    "len(new_auths), new_auths[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we don't want a list with only one element, we will add an additional step so that we only need to append one string when the list has only one element and NaN when it has non."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new list\n",
    "\n",
    "new_lang = []\n",
    "\n",
    "\n",
    "for i in df_final.language:\n",
    "    tmp = []\n",
    "    if pd.notna(i) == True:\n",
    "        val = i.split(';')\n",
    "        for s in val:\n",
    "            val_2 = s.strip()#.strip('&amp').strip('nbspr\\n').strip()\n",
    "            tmp.append(val_2)\n",
    "        join_them = list(filter(None, tmp))\n",
    "        #new_lang.append(join_them)\n",
    "        if len(join_them) == 1:\n",
    "            new_lang.append(join_them[0])\n",
    "        else:\n",
    "            new_lang.append(join_them[0])\n",
    "    else:\n",
    "        new_lang.append(np.nan)\n",
    "        \n",
    "len(new_lang), new_lang[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the the new variables to the dataset\n",
    "df_final = df_final.drop(columns=['author_s', 'language'])\n",
    "df_final.insert(4, 'author_s', new_auths, True)\n",
    "df_final.insert(7, 'language', new_lang, True)\n",
    "df_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the dataset\n",
    "\n",
    "To save our new dataset we will use UTF-8 encoding to account with the different spellings encountered throughout the data. We can do this by adding as an argument the **encoding='utf-8'** option in pd.to_csv()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_csv('your path/full_data.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('your path/full_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that the saving the data was done correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df.columns), len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
