def has_banner_class_and_hello_world(tag):
    return tag.attr('class') == "banner" and tag.string == "Hello world"

soup.find_all(has_banner_class_and_hello_world)


for ids
soup.select("#selected")

prefix
"http://www.ifar.org/"

when we use beautifulsoup and need text only use
.get_text()
whatever you put in the function above, as a string, will be used to separate each content


--------------------------- From DataCamp -------------------------------------

Use xpath to navigate to a very specific place in the html file

xpath = 'hmtl/div/p[2]'

You can navigate to all of the elements of a table or something similar with '//table'

xpath = '//table'

You can combine both of the above as well to navigate to all of the tables that are descendant of that div element

xpath = 'hmtl/div/p[2]//table'

You can go further with XPaths by putting what you want within a tag --e.g. a specific class of that tag-- inside brackets

xpath = '//span[@class="span-class"]'

You can also use a * wildcard to select everything after a tag. It will select everything afterwards regardless of which tag it is.

xpath = '/html/body/*'


A useful function to check the text of elements

def print_element_text( xpath ):
  text = ' '.join( sel.xpath( xpath ).xpath( './text()' ).extract() )
  print( text )
  
Characters such as @ refer to attributes

We could also combine wildcards with @ attributes to get straight to were we would like to get

xpath = '//*[@is="uid"]'

or you could get even more specific with

xpath = '//*[@is="uid"]/p[2]'

Also check, contains with attributes

contains(@attri-name, "string-expres")
xpath = '//*[contains(@class, "class-1)]'

The difference with the above and xpath = '//*[@class="class-1"]' is that the latter will only pick elements that match entirely to class-1 while the former will make sure the class or attribute is just contained in the element

To go directly to the attribute you just need to put is at the end of an expression

xpath = 'hmtl/div/p[2]/@class'

If you'd like to print the attribute for a given path, use:

def print_attribute( xpath ):
  print( "You have selected:" )
  for i,el in enumerate(sel.xpath( xpath ).extract()):
        print( "%d) %s" % (i+1, el) )
     
    
To select the hyperlinks use

xpath = '//p[@id="p2"]/a/@href'

More useful formulas:

def how_many_elements( xpath ):
  print( "You've selected %d elements" % len(sel.xpath( xpath )) )
  
def preview( xpath ):
  els = sel.xpath( xpath ).extract()
  n = len(els)
  for i,el in enumerate( els[:min(4,n)]):
    print( "Element %d: %s" % (i+1,el) )
    
    
Very useful xpath to be used with the formulas above

xpath = '//a[contains(@class,"course-block")]/@href'

--------------------------- CSS Locators----------------------------

These locators use a > as opposed to the /

e.g. '/html/body/div' would be 'html > body > div'

same with the double space '//div/span//p' is now 'div > span p' notice that two slashes are completely gone

[N] is replaced by ':nth-of-type(N)'

for example '//div/p[2]' is now 'div > p:nth-of-type(N)'

To find an element by a class use a '.' e.g. 'p.class-1'
To find an element by aN ID use a '#' e.g. 'div#uid'

Example with both

css = 'div#uid > p.class-1'

Select all elements in the document which class belongs to 'class1'

css = '.class1'

This example will take you to all elements in the document regardless if they have an additional class

You can use the same combo with scrapy e.g. sel.css(' div > p')


The CSS Locator string '*' selects all elements in the HTML document.
The CSS Locator string '*.class-1' selects all elements which belong to class-1, but this is unnecessary since the string '.class-1' will also do the same job.
The CSS Locator string '*#uid' selects the element with id attribute equal to uid, but this is unnecessary since the string '#uid' will also do the same job.

Create the CSS Locator to all children of the element whose id is uid
css_locator = '#uid > *'


To get the href of an id of uid with xpath and css

xpath = '//div[@id="uid"]/a/@href'

the # tells us to select the div by its id='uid' while the > tells us to move down one generation and ::atr() is to select the a's attribute
css_locator = 'div#uid > a::attr(href)'

We can use text() inside the xpath to select the text of our desired elements or tags.
sel.xpath('//p[@id="p-example"]/text()').extract()

Conversely, a // before the text would get all of the text in the example
sel.xpath('//p[@id="p-example"]//text()').extract

Same for css but now with '::text' or ' ::text' for all of it
sel.css('p#p-example::text').extract()

sel.css('p#p-example ::text').extract()

Useful formula

def print_results( xpath, css_locator ):
  print( "Your XPath extracts to following:")
  print( our_xpath(xpath) )
  print("_________________\n")
  print( "Your CSS Locator extracts the following:")
  print( our_css(css_locator) )
  return None
  
  
-------Response-------

The variable response gets assigned a Response object that can use any xpath or css method on it. In addition, it can also use url and all sorts of methods
this_url = response.url
this_title = response.css('html > head > title::text').extract_first()

Selector and Response objects both return a SelectorList when using the xpath or css methods to direct to elements.

-------------------Building the crawler-------------------------

Useful function

def inspect_class(c):
  newc = c()
  meths = dir(newc)
  if 'name' in meths:
    print("Your spider class name is:", newc.name)
  if 'from_crawler' in meths:
    print("It seems you have inherited methods from scrapy.Spider -- NICE!")
  else:
    print("Oh no! It doesn't seem that you are inheriting the methods from scrapy.Spider!!")
    
class DCspider( scrapy.Spider ):
    name = "dcspider"

    def start_requests( self ):
        urls = [ 'https://www.datacamp.com/courses/all' ]
        for url in urls:
            yield scrapy.Request( url = url, callback = self.parse )

    def parse( self, response ):
        links = response.css('div.course-block > a::attr(href)').extract()
        for link in links:
            yield response.follow( url = link, callback = self.parse2 )

    def parse2( self, response ):
    # parse the course sites here!
    
    
inspect_spider
def inspect_spider( s ):
  news = s()
  try:
    req1 = list( news.start_requests() )[0]
    html1 = requests.get( req1.url ).content
    response1 = TextResponse( url = req1.url, body = html1, encoding = 'utf-8' )
    req2 = list( news.parse( response1 ) )[0]
    html2 = requests.get( req2.url ).content
    response2 = TextResponse( url = req2.url, body = html2, encoding = 'utf-8' )
    for d in news.parse_descr( response2 ):
      print("One course description you found is:", d )
      break
  except:
    print("Oh no! Something is wrong with the code. Keep trying!")